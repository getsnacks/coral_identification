# -*- coding: utf-8 -*-
"""coral_identification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cFxaUedWWfQfFp8njFFlQ-K9KLlABfl4

Data collection and Preprocessing
"""

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
import numpy as np

batch_size = 32
img_size = 244
epochs = 10
channels = 3

import os
print(os.getcwd())  # Print current working directory
print(os.listdir())  # List files and directories in the current directory

import zipfile

with zipfile.ZipFile('coral_dataset_merged.zip', 'r') as zip_ref:
    zip_ref.extractall('/content')

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    'coral_dataset_merged',
    shuffle=True,
    image_size=(img_size, img_size),
    batch_size=batch_size
)

class_names = dataset.class_names
class_names

len(dataset)

num_batches = len(list(dataset.as_numpy_iterator()))
print(f"Number of batches in dataset: {num_batches}")


for image_batch, label_batch in dataset.take(1):
    batch_size = image_batch.shape[0]
    print(f"Batch size: {batch_size}")

    total_images = num_batches * batch_size
print(f"Total number of images in dataset: {total_images}")

data_list = list(dataset.as_numpy_iterator())
print(f"Total number of items in dataset: {len(data_list)}")

plt.figure(figsize=(10,10))
for image_batch, label_batch in dataset.take(1):
  for i in range(12):
      ax = plt.subplot(3,4,i+1)
      plt.imshow(image_batch[i].numpy().astype('uint8'))
      plt.title(class_names[label_batch[0]])
      plt.axis('off')

#%80 --> training
#%20 --> %10 validation, %10 test

train_size = 0.8
len(dataset)*train_size

train_dataset = dataset.take(23)
len(train_dataset)

test_dataset = dataset.skip(23)
len(test_dataset)

val_size = 0.1
len(dataset)*val_size

val_dataset = test_dataset.take(2)
len(val_dataset)

test_dataset = test_dataset.skip(2)
len(test_dataset)

def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
    dataset_size = len(ds)
    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * dataset_size)
    val_size = int(val_split * dataset_size)
    train_dataset = ds.take(train_size)
    val_dataset = ds.skip(train_size).take(val_size)
    test_dataset = ds.skip(train_size).skip(val_size)

    return train_dataset, val_dataset, test_dataset

train_dataset, val_dataset, test_dataset = get_dataset_partitions_tf(dataset)

len(train_dataset)

len(val_dataset)

len(test_dataset)

train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

import tensorflow as tf
from tensorflow.keras import layers

new_var = tf.keras.Sequential

resize_and_rescale = new_var([
    layers.Resizing(img_size, img_size),
    layers.Rescaling(1.0/224)
])

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
])
data_augmentation

"""Building and Training"""

img_size= 244
channels = 3   # Number of channels (e.g., 3 for RGB images)
n_classes = 2

model = models.Sequential([
    layers.Input(shape=(img_size, img_size, channels)),
    resize_and_rescale,
    layers.Conv2D(32, kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
])

model.summary()

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_dataset,
    batch_size=batch_size,
    validation_data=val_dataset,
    verbose=1,
    epochs=30,
)

scores = model.evaluate(test_dataset)

scores

history

history.params

history.history.keys()

history.history['accuracy']

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

print(len(acc))
print(len(val_acc))

epochs = len(acc)

plt.figure(figsize=(7, 7))
plt.subplot(1, 2, 1)
plt.plot(range(epochs), acc, label='Training Accuracy')
plt.plot(range(epochs), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(range(epochs), loss, label='Training Loss')
plt.plot(range(epochs), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""Running a prediction on a sample image"""

import numpy as np
for images_batch, labels_batch in test_dataset.take(1):

    first_image = images_batch[0].numpy().astype('uint8')
    first_label = labels_batch[0].numpy()

    print("first image to predict")
    plt.imshow(first_image)
    print("actual label:",class_names[first_label])

    batch_prediction = model.predict(images_batch)
    print("predicted label:",class_names[np.argmax(batch_prediction[0])])

"""Write a function for inference"""

def predict(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

plt.figure(figsize=(15, 15))
for images, labels in test_dataset.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = predict(model, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")

"""Saving the model"""

import os

model_dir = "../models"

if not os.path.exists(model_dir):
    os.makedirs(model_dir)

model_version = max([int(i.split('.')[0]) for i in os.listdir(model_dir) if i.endswith('.keras')] + [0]) + 1

model.save(f"{model_dir}/{model_version}.keras")